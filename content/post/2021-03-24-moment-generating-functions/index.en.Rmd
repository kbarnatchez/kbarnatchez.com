---
title: Moment-Generating Functions
author: Keith Barnatchez
date: '2021-03-24'
slug: moment-generating-functions
categories:
  - Statistics
tags:
  - statistics
subtitle: ''
summary: ''
authors: []
lastmod: '2021-03-24T11:39:24-04:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

For this post, I'll assume you have a basic understanding of probability distributions and their moments. If you want a really good overview of *moments*, I highly recommend [this post](https://gregorygundersen.com/blog/2020/04/11/moments/) by Gregory Gundersen.

## What is a moment-generating function?

We'll start with the general definition. Given a random variable $Y$, its associated moment-generating function $m_Y$ is defined such that
$$
m_Y(t) = \mathbb{E}(e^{tY})
$$
And **what do they do?** This may be a bit shocking, but we tend to use moment-generating functions to *generate the moments* of a random variable -- any moment that we want! Just from looking at the definition, it isn't super clear how the above expectation allows us to obtain any desired moment. To make this clearer, we have to do a bit of work.

You may remember that the Taylor Series expansion of $e^{ty}$ is given by
$$
e^{ty} = 1 + \frac{ty}{1!} + \frac{(ty)^3}{3!} + \frac{(ty)^2}{2!} + \cdots
$$
Writing out our earlier expectation, we obtain
$$
\begin{aligned}
\mathbb{E}(e^{tY}) &= \sum_y \left(1 + \frac{ty}{1!} + \frac{(ty)^3}{3!} + \frac{(ty)^2}{2!} + \cdots\right) p(y) \\
&= \sum_y p(y) + \sum_y p(y)\frac{ty}{1!} + \sum_y p(y)\frac{(ty)^2}{2!} + \sum_y p(y)\frac{(ty)^3}{3!} + \cdots \\
&= \sum_y p(y) + t\sum_y p(y)y + \frac{t^2}{2!}\sum_y p(y)y^2 + \frac{t^3}{3!}\sum_y p(y)y^3 + \cdots \\
&= 1 + t \mathbb{E}(Y) +  \frac{t^2}{2!} \mathbb{E}(Y^2) + \frac{t^3}{3!} \mathbb{E}(Y^3) + \cdots
\end{aligned}
$$
Recall that the $k$-th *uncentered moment* of a random variable $Y$, also called the $k$-th moment about the origin, is given by $\mathbb{E}(Y^k)$. Okay, so we can see these terms appearing in the infinite sum, but they still have these $t$'s and factorials attached to them. So how do we make the final jump that allows us to obtain these moments?

Notice that 
$$
\begin{aligned}
\frac{d}{dt} \mathbb{E}(e^{tY}) \bigg\rvert_{t=0} &= \left(0 + \mathbb{E}(Y)  + t \mathbb{E}(Y^2) + \frac{t^2}{2!}\mathbb{E}(Y^3) + \cdots\right)\bigg\rvert_{t=0} \\
&= 0 + \mathbb{E(Y)} + 0 + 0 + \cdots \\
&= \mathbb{E}(Y)
\end{aligned}
$$
and
$$
\begin{aligned}
\frac{d^2}{dt^2} \mathbb{E}(e^{tY}) \bigg\rvert_{t=0} &= \left(0 + 0  + \mathbb{E}(Y^2) + t\mathbb{E}(Y^3) + \cdots\right)\bigg\rvert_{t=0} \\
&= 0 + 0 + \mathbb{E(Y^2)} + 0 + \cdots \\
&= \mathbb{E}(Y^2)
\end{aligned}
$$
and 
$$
\begin{aligned}
\frac{d^3}{dt^3} \mathbb{E}(e^{tY}) \bigg\rvert_{t=0} &= \left(0 + 0  + 0 + t\mathbb{E}(Y^3) + \cdots\right)\bigg\rvert_{t=0} \\
&= 0 + 0 + 0 + \mathbb{E(Y^3)} + 0 + \cdots \\
&= \mathbb{E}(Y^3)
\end{aligned}
$$
and from here, the pattern is pretty clear: if we take the $k-th$ derivative of $\mathbb{E}(e^{tY})$ and evaluate it at 0, we obtain $Y$'s $k-th$ uncentered moment. We can obtain *centered* moments using the same approach pretty easily: just obtain the first uncentered moment $E(Y)=\mu_Y$, then define $Z=Y-\mu_Y$ and calculate the *uncentered* moments of $Z$ (which are really centered since $\mathbb{E}(Z) = \mathbb{E}(Y) - \mu_Y = 0$) via its MGF. 

### The uniqueness of moment-generating functions

The main appeal from moment-generating functions comes from the following result:

> *If $Y_1$ and $Y_2$ are such that $m_{Y_1}(t) = m_{Y_2}(t)$ for some region around $t=0$, then $Y_1$ and $Y_2$ follow the same distribution.*

This makes sense when we think about it (though it's not so easy to prove). We know that a distribution is uniquely characterized by its moments: if we can quantify its center, spread, and the relative thickness of its tails, then we know everything there is to know about that distribution. Thus, two variables have the same moment-generating functions, then they have the same moments and have to follow the same distribution. 

## Why should we care?
 
There are a couple instances where moment-generating functions can be super useful:

1. Proving a random variable follows some distribution
2. Deriving the density function of a function of another variable whose moment-generating function is known

We can work through an example where moment-generating functions make our lives easier. Suppose $Y_1$ and $Y_2$ are independent Poisson random variables with rates $\lambda_1$ and $\lambda_2$, and that we're asked to 1) find the density function of $Y_1 + Y_2$, and 2) find its expected value and variance. It turns out, moment-generating functions are often great choices finding the distributions of sums of independent random variables.

Since $Y_1$ and $Y_2$ are independent, we know that 
$$
\begin{aligned}
\mathbb{E}(e^{t(Y_1+Y_2)}) &= \mathbb{E}(e^{tY_1}) \mathbb{E}(e^{tY_2}) \\
&= m_{Y_1}(t) \times m_{Y_2}(t)
\end{aligned}
$$
For now, we can be lazy and use the fact that the MGF for a Poisson random variable is $e^{\lambda(e^t-\lambda)}$, though this isn't so hard to derive ourselves. Thus,
$$
\begin{aligned}
m_{Y_1+Y_2}(t) &= e^{\lambda_1(e^t-\lambda_1)} e^{\lambda_2(e^t-\lambda_2)} \\
&= e^{(\lambda_1+\lambda_2)(e^t - (\lambda_1+\lambda_2))}
\end{aligned}
$$
We can see that this is just the MGF for a Poisson random variable with rate $\lambda_1+\lambda_2$ -- thus, the sum of two Poisson random variables is also a Poisson random variable with rate equal to the sum of the individual rates:
$$
f_{Y_1+Y_2}(y) = \frac{(\lambda_1+\lambda_2)^y e^{-(\lambda_1+\lambda_2)}  }{y!}
$$
Now, 
$$
m_{Y_1+Y_2}^{(1)}(0)= \lambda_1+\lambda_2 
$$
and 
$$
m_{Y_1+Y_2}^{(2)}(0)= (\lambda_1+\lambda_2)^2 +  \lambda_1+\lambda_2 
$$
so that $\mathbb{E}(Y_1+Y_2)=\lambda_1+\lambda_2$ and 
$$
\mathbb{V}(Y_1+Y_2)= (\lambda_1+\lambda_2)^2 +  \lambda_1+\lambda_2 -  (\lambda_1+\lambda_2)^2 =  \lambda_1+\lambda_2
$$
