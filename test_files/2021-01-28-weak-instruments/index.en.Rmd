---
title: Weak instruments
author: Keith Barnatchez
date: '2021-01-28'
slug: weak-instruments
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2021-01-28T09:36:11-05:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(ggplot2)
library(mvtnorm)
```

A common challenge in causal inference is the presence of unobserved confounding variables. Imagine we're interested in the average effect some variable $x_1$ has on some outcome of interest $y$. We're able to collect data on $x_1$ and $y$, but it turns out that the true data generating process is
$$
y = \beta x_1 + \alpha x_2 + \varepsilon
$$
where
$$
(x_1,x_2) \sim N\left(
\begin{bmatrix}
0 \\ 0
\end{bmatrix},
\Sigma
\right)
$$
and $\varepsilon \sim N(0,\sigma^2_\varepsilon)$. If we were to go ahead and just estimate the model
$$
\hat{y} = \hat{\beta}_0 + \hat{\beta}x_1
$$
we would run into issues in the event that there is some correlation between $x_1$ and $x_2$ and $\alpha \neq 0$. The intuition is that, since movements in $x_1$ are correlated with movements in $x_2$ but only observe $x_1$, we give all the "credit" of (). 

Statisticians interested in causal inference have spent loads of time thinking about ways to circumvent the above issue. One common approach is the method of instrumental variables. The idea is that if $y$ is generated as described above, but there is some variable $z$ that is correlated with $x_1$ and uncorrelated with our other confounders, then we can use $z$ to "issolate" movements in $x_1$ that are unrelated to movements in $x_2$. 

In practice, there are a few ways to implement instrumental variables estimation. Perhaps the most intuitive is the method of two-stage least squares. 

1. Estimate 
$$
\hat{x}_1 = \hat{\alpha}_0 + \hat{\alpha}_1  z
$$
2. Take your fitted values $\hat{x_1}$ and estimate
$$
\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 \hat{x}_1
$$

## Practial issues

As we'll see at the end of the post, the instrumental variables approach can have nice properties **provided that** (1) our instrument is truly uncorrelated with the other confounders and (2) it's actually correlated with $x_1$. And it turns out, there can be issues if correlation exists between $x_1$ and $z$, but it is very weak. First, let's demonstrate an example of. We'll make the following assumptions:

1. $\text{corr}(x_1,z)=0.7$
2. $\text{corr}(x_2,z)=0$
3. $\text{corr}(x_1,x_2)=-0.5$

```{r simulate, echo=FALSE}
minN <- 50
maxN <- 5000

cor_x1_z <- 0.7
cor_x1_x2 <- -0.5
cor_x2_z <- 0.0

sigma <- matrix(c(1,cor_x1_x2,cor_x1_z,
                  cor_x1_x2,1,cor_x2_z,
                  cor_x1_z,cor_x2_z,1),ncol = 3)

beta_2s <- rep(NA,maxN-minN+1) 
beta    <- rep(NA,maxN-minN+1)

# Loop 
i <- 1
for (n in minN:maxN) {

  X <- rmvnorm(n,mean=c(0,0,0),sigma=sigma, method="eigen")
  x1 <- X[,1]
  x2 <- X[,2]
  z <-  X[,3]
  
  y <- 0.3*x1 - 0.7*x2 + rnorm(n) 
  xhat <- lm(x1 ~ z)$fitted.values
  
  beta[i] <- lm(y ~ x1)$coefficients[2]
  beta_2s[i] <- lm(y ~ xhat)$coefficients[2]
  i <- i+1

}

# Make dfs
df_raw <- data.frame(n=minN:maxN,beta=beta,Version=rep('Unadjusted',maxN-minN+1))
df_2s  <-  data.frame(n=minN:maxN,beta=beta_2s,Version=rep('Two-stage',maxN-minN+1))
df <- rbind(df_raw,df_2s)
```

```{r plot, echo=FALSE}
ggplot(df, aes(x=n,y=beta,color=Version)) + geom_line() + geom_hline(yintercept = 0.3) +      labs(x='Sample size',
       y = 'Effect estimate',
       title='Regular vs. two-stage least squares',
       subtitle='Assuming presence of strong instrument') +
  scale_y_continuous(limits=c(-0.5,1))
```

A couple things are apparent right away. Unsurprisingly, the variance of both estimates falls as the sample size increases. Notice that the two-stage approach has slightly higher variance as a result of the additional uncertainty introduced by the first stage estimation Importantly, we can see mean estimates of both approaches differ by a constant -- this value is the bias that we'll derive later. And perhaps most importantly, we can see that the two-stage estimates are centered around the true value of $\beta$ (the black reference line) -- this reflects the unbiasedness of two-stage least squares when the necessary conditions hold. 

Now we'll consider a case where $z$ is a weak instrument. We'll mostly use the same assumptions as above, but now suppose $\text{corr}(x_1,z)=0.1$. Repeating the same procedure under this new assumption yields the following:

```{r simulate2, echo=FALSE}
minN <- 50
maxN <- 5000

cor_x1_z <- 0.1
cor_x1_x2 <- -0.5
cor_x2_z <- 0.0

sigma <- matrix(c(1,cor_x1_x2,cor_x1_z,
                  cor_x1_x2,1,cor_x2_z,
                  cor_x1_z,cor_x2_z,1),ncol = 3)

beta_2s <- rep(NA,maxN-minN+1) 
beta    <- rep(NA,maxN-minN+1)

# Loop 
i <- 1
for (n in minN:maxN) {

  X <- rmvnorm(n,mean=c(0,0,0),sigma=sigma, method="eigen")
  x1 <- X[,1]
  x2 <- X[,2]
  z <-  X[,3]
  
  y <- 0.3*x1 - 0.7*x2 + rnorm(n) 
  xhat <- lm(x1 ~ z)$fitted.values
  
  beta[i] <- lm(y ~ x1)$coefficients[2]
  beta_2s[i] <- lm(y ~ xhat)$coefficients[2]
  i <- i+1

}

# Make dfs
df_raw <- data.frame(n=minN:maxN,beta=beta,Version=rep('Unadjusted',maxN-minN+1))
df_2s  <-  data.frame(n=minN:maxN,beta=beta_2s,Version=rep('Two-stage',maxN-minN+1))
df <- rbind(df_raw,df_2s)
```

```{r plot2, echo=FALSE}
ggplot(df, aes(x=n,y=beta,color=Version)) + geom_line() + geom_hline(yintercept = 0.3) +   labs(x='Sample size',
       y = 'Effect estimate',
       title='Regular vs. two-stage least squares',
       subtitle='Assuming presence of weak instrument') +
       scale_y_continuous(limits=c(-0.5,1))
```



## Isolating the bias

If we let $X_1$ and $X_2$ represent vectors of individual observations on $x_1$ and $x_2$, then if the true model is
$$
y = X_1\beta + X_2\alpha + U
$$
and we estimate
$$
y = X_1\beta + E
$$
then our estimate $\hat{\beta}$ is given by:
$$
\hat{\beta} = (X_1' X_1)^{-1}X_1'Y
$$
Now, we'll sub in the true model for $Y$ to let us relate $\hat{\beta}$ to $\beta$:
$$
\begin{align}
\hat{\beta} &= (X_1' X_1)^{-1}X_1'(X_1\beta + X_2\alpha + U) \\ 
&= \underbrace{(X_1' X_1)^{-1}X_1'X_1}_{I}\beta + (X_1' X_1)^{-1}X_1' X_2\alpha + (X_1' X_1)^{-1}X_1'U \\
&= \beta + (X_1' X_1)^{-1}X_1' X_2\alpha + (X_1' X_1)^{-1}X_1'U
\end{align}
$$
Then, we just have to take the expected value of both sides to derive the expected bias:
$$
\begin{align}
\mathbb{E}(\hat{\beta}) &= \mathbb{E}(\beta) + \mathbb{E}[ (X_1' X_1)^{-1}X_1' X_2\alpha] + \underbrace{\mathbb{E}[ (X_1' X_1)^{-1}X_1'U]}_{0}  \\
&= \beta + \underbrace{\alpha (X_1'X)^{-1}\mathbb{E}(X_1'X_2|X_1)}_{\text{bias}}
\end{align}
$$
On the first line, the final term drops out. Why's that, if the whole issue is that $X_1$ is correlated with the error term? Recall that $U$ corresponds to the errors of the true model, where our assumption of $\text{corr}(X_1,U)=0$ actually does hold, unlike our assumption of $\text{corr}(X_1,E)$. On the second line, I can move $\alpha$ to the front since it's just a scalar, and I emphasize that the remiaining expectation term is conditional on $X_1$, which we observe.

So what does it take for $\mathbb{E}(\hat{\beta})=\beta$? Effectively, we need the second term in the expression above to drop out. There are two sufficient conditions:

1. $\alpha=0$. In this scenario, even if $X_1$ and $X_2$ are correlated, it won't impact out inferences since $X_2$ doesn't influence $y$.
2. $\text{corr}(X_1,X_2)=0$. In this case, even though $X_2$ influences $y$, we don't conflate ()

## Unbiasedness of two-stage least squares

To show the unbiasedness of the two-stage least squares approach, let's assume there is some variable $Z$ such that $\text{corr}(X_1,Z)\neq 0$ and $\text{corr}(X_2,Z)=0$. We'll let $\hat{\gamma}$ be the coefficient from regressing $X_1$ on $Z$:
$$
\hat{X}_1=\hat{\gamma} Z
$$
Then, we can proceed in a similar fashion to before:
$$
\begin{align}
\hat{\beta}&=(\hat{\gamma} Z' \hat{\gamma} Z)^{-1}\hat{\gamma} Z'(X_1\beta + X_2\alpha + U) \\
&= (\hat{\gamma} Z' \hat{\gamma} Z)^{-1}\hat{\gamma} Z'X_1\beta + (\hat{\gamma} Z' \hat{\gamma} Z)^{-1}\hat{\gamma} Z'X_2\alpha + (\hat{\gamma} Z' \hat{\gamma} Z')^{-1}\hat{\gamma} Z'U \\
&= \frac{1}{\hat{\gamma}}\underbrace{\left(Z'Z\right)^{-1}Z'X_1}_{\hat{\gamma}} \beta + (\hat{\gamma} Z' \hat{\gamma} Z)^{-1}\hat{\gamma} Z'X_2\alpha + (\hat{\gamma} Z' \hat{\gamma} Z)^{-1}\hat{\gamma} Z'U \\
&= \beta + (\hat{\gamma} Z' \hat{\gamma} Z)^{-1}\hat{\gamma} Z'X_2\alpha + (\hat{\gamma} Z' \hat{\gamma} Z)^{-1}\hat{\gamma} Z'U
\end{align}
$$
Taking expectations again (and noting that the last term drops out since $Z$ and $U$ are uncorrelated):
$$
\begin{align}
\mathbb{E}(\hat{\beta}) &= \beta + \mathbb{E}[(\hat{\gamma} Z' \hat{\gamma} Z)^{-1}\hat{\gamma} Z'X_2\alpha] \\ 
&=\beta + \frac{ \alpha}{\gamma} \mathbb{E}[(Z'Z)^{-1}Z'X_2 ] \\
&= \beta + \frac{ \alpha \delta}{\gamma}
\end{align}
$$
where I let $\delta=\mathbb{E}[(Z'Z)^{-1}Z'X_2 ]$. Given our assumption that $\text{corr}(X_2,Z)=0$, $\delta=0$ and the final term drops out. 

I like this last expression because it lets us think of scenarios where 